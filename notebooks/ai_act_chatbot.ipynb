{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot for Technical Documentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate how to build a RAG (Retrieval-Augmented Generation) chatbot for technical documentation. We used the the *Artificial Intelligence Act* (https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf ) as the technical documentation for this demonstration.\n",
    "\n",
    "Our approach follows the steps below:\n",
    "1. Split the document into chunks of text\n",
    "2. Generate and store the embeddings for each chunk\n",
    "3. Create a retriever model to find the most relevant chunks for a given question\n",
    "4. Initialize the LLM and prompt template\n",
    "5. Define RAG chain\n",
    "5. Invoke RAG chain\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "To reproduce the following notebook, you need to install the following libraries/packages:\n",
    "```bash\n",
    "pip install transformers\n",
    "pip install torch\n",
    "pip install faiss-cpu\n",
    "pip install langchain\n",
    "pip install langchain_huggingface\n",
    "```\n",
    "\n",
    "Additionally, make sure that you have a data folder with the PDF document in it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from transformers import AutoModelForCausalLM, pipeline, AutoTokenizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import faiss \n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "READER_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "FAISS_INDEX_PATH = \"../embeddings/knowledge_vector_database.faiss\"\n",
    "PDF_FILE_PATH = \"../data/raw/TA-9-2024-0138_EN.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the document into chunks\n",
    "def split_document_into_chunks(file_path: str, chunk_size: int, tokenizer_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load a document and split it into smaller chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "        chunk_size (int): The maximum size of each chunk (number of tokens).\n",
    "        tokenizer_name (str): The name of the tokenizer to use for splitting the document.\n",
    "\n",
    "    Returns:\n",
    "        List of split document chunks.\n",
    "    \"\"\"\n",
    "    # Check if the document file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"The file '{file_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the document using PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    logging.info(f\"The document has been loaded successfully. Total number of pages: {len(pages)}.\")\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1),  \n",
    "        add_start_index=True, \n",
    "        strip_whitespace=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    logging.info(f\"The document has been split into {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 09:58:52,541 - INFO - The document has been loaded successfully. Total number of pages: 459.\n",
      "2024-11-03 09:58:54,117 - INFO - The document has been split into 814 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='European Parliament\n",
      "2019-2024\n",
      "TEXTS ADOPTED\n",
      "P9_TA(2024)0138\n",
      "Artificial Intelligence Act\n",
      "European Parliament legislative resolution of 13 March 2024 on the proposal for a \n",
      "regulation of the European Parliament and of the Council on laying down harmonised \n",
      "rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union \n",
      "Legislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD))\n",
      "(Ordinary legislative procedure: first reading)\n",
      "The European Parliament,\n",
      "– having regard to the Commission proposal to Parliament and the Council \n",
      "(COM(2021)0206),\n",
      "– having regard to Article 294(2) and Articles 16 and 114 of the Treaty on the \n",
      "Functioning of the European Union, pursuant to which the Commission submitted the \n",
      "proposal to Parliament (C9-0146/2021),\n",
      "– having regard to Article 294(3) of the Treaty on the Functioning of the European Union,' metadata={'source': '../data/raw/TA-9-2024-0138_EN.pdf', 'page': 0, 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_document_into_chunks(PDF_FILE_PATH, 256)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for the document chunks\n",
    "def generate_embeddings(chunks: list):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given document chunks and store them using FAISS (uses the nearest neighbor search algorithm).\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks to generate embeddings for.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS object containing the document embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cpu\"},  # Use CPU for embeddings\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    logging.info(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "    # Generate embeddings for the document chunks \n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "        chunks, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Embeddings generated successfully.\")\n",
    "\n",
    "    return KNOWLEDGE_VECTOR_DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the FAISS object containing the document embeddings to a file\n",
    "def save_knowledge_vector_database(knowledge_vector_database, file_path):\n",
    "    \"\"\"\n",
    "    Save the FAISS object containing the document embeddings to a file.\n",
    "    Args:\n",
    "        knowledge_vector_database (FAISS): FAISS object containing the document embeddings.\n",
    "        file_path (str): Path to save the knowledge vector database.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(knowledge_vector_database, f)\n",
    "    logging.info(f\"Knowledge vector database saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We use the functions implemented above to split the document into chunks, generate embeddings for each chunk, and store them. We also ensure that the directory for saving the knowledge vector database exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 09:58:55,538 - INFO - The document has been loaded successfully. Total number of pages: 459.\n",
      "2024-11-03 09:58:56,931 - INFO - The document has been split into 814 chunks.\n",
      "2024-11-03 09:58:57,007 - INFO - Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "2024-11-03 09:58:59,063 - INFO - Embedding model 'thenlper/gte-small' loaded successfully.\n",
      "2024-11-03 09:58:59,065 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-11-03 09:58:59,065 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2024-11-03 09:59:54,209 - INFO - Embeddings generated successfully.\n",
      "2024-11-03 09:59:54,586 - INFO - Knowledge vector database saved to ../embeddings/knowledge_vector_database.faiss\n"
     ]
    }
   ],
   "source": [
    "# Ensure the directory for saving the knowledge vector database exists\n",
    "os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)\n",
    "\n",
    "chunks = split_document_into_chunks(PDF_FILE_PATH, chunk_size=256)\n",
    "if chunks is not None:\n",
    "    # Generate embeddings for the document chunks\n",
    "    knowledge_vector_database = generate_embeddings(chunks)\n",
    "    # Save the entire knowledge vector database to a file\n",
    "    save_knowledge_vector_database(knowledge_vector_database, FAISS_INDEX_PATH)\n",
    "else:\n",
    "    logging.error(\"Failed to split the document into chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the reader model\n",
    "def initialize_reader_model(model_name: str = READER_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Initialize the LLM model for text generation.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model to use for the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        A A HuggingFace pipeline for text generation and the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\")    \n",
    "\n",
    "    # Initialize the reader LLM model\n",
    "    reader_llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    "    )\n",
    "    logging.info(f\"Reader LLM model '{model_name}' initialized successfully.\")\n",
    "    return reader_llm, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the entire knowledge vector database from a file\n",
    "def load_knowledge_vector_database(file_path):\n",
    "    \"\"\"\n",
    "    Load the entire FAISS object from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the knowledge vector database file.\n",
    "\n",
    "    Returns:\n",
    "        The loaded FAISS object.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            knowledge_vector_database = pickle.load(f)\n",
    "        logging.info(f\"Knowledge vector database loaded from {file_path}\")\n",
    "        return knowledge_vector_database\n",
    "    else:\n",
    "        logging.error(f\"Knowledge vector database file {file_path} does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant documents from the knowledge base\n",
    "def retrieve_relevant_docs(query: str, knowledge_vector_database, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents from the FAISS knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        knowledge_vector_database: The FAISS knowledge base for retrieval.\n",
    "        k (int): The number of top documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing the retrieved documents, their combined text, and their metadata.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting retrieval for query: {query}\")\n",
    "    retrieved_docs = knowledge_vector_database.similarity_search(query=query, k=k)\n",
    "\n",
    "    retrieved_docs_text = [doc.page_content for doc in retrieved_docs]\n",
    "    retrieved_docs_metadata = [doc.metadata for doc in retrieved_docs]\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {i}:::\\n{doc}\\n\" for i, doc in enumerate(retrieved_docs_text)])\n",
    "\n",
    "    return retrieved_docs, context, retrieved_docs_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the final answer using the retrieved documents and LLM\n",
    "def generate_answer_from_docs(query: str, context: str, reader_llm, tokenizer, retrieved_docs_metadata, max_new_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate an answer using the LLM based on the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        context (str): The text of the retrieved documents.\n",
    "        reader_llm: The text generation pipeline (LLM).\n",
    "        tokenizer: The tokenizer for formatting the chat-based prompt.\n",
    "        retrieved_docs_metadata (list): Metadata of the retrieved documents.\n",
    "        max_new_tokens (int): Maximum number of tokens for the generated answer.\n",
    "\n",
    "    Returns:\n",
    "        The generated answer from the LLM, including the page numbers of the retrieved chunks.\n",
    "    \"\"\"\n",
    "    # Chat-style prompt for the model - prompt template\n",
    "\n",
    "    prompt_in_chat_format = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Avoid referencing specific document names, such as \"According to Document 0\". \n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context:\n",
    "            {context}\n",
    "            ---\n",
    "            Now here is the question you need to answer:\n",
    "\n",
    "            Question: {query}\"\"\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template to the prompt\n",
    "    rag_prompt_template = tokenizer.apply_chat_template(\n",
    "        prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate the final answer\n",
    "    generated_text = reader_llm(rag_prompt_template, truncation=True, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Process the generated text\n",
    "    answer = generated_text[0]['generated_text']\n",
    "\n",
    "    # Extract page numbers from metadata\n",
    "    page_numbers = sorted(set([metadata['page'] for metadata in retrieved_docs_metadata]))\n",
    "    page_numbers_str = \", \".join(map(str, page_numbers))\n",
    "\n",
    "    # Append page numbers to the answer\n",
    "    answer += f\"\\n\\nPages retrieved from the document and included in the context for generating the answer: {page_numbers_str}\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering Pipeline\n",
    "\n",
    "In this section, we load the precomputed **FAISS object** (knowledge vector database) to enable the retrieval of relevant information from the document. If loading fails, an error is logged, and the script exits.\n",
    "\n",
    "Once the knowledge base is loaded successfully, we initialize the **Reader LLM model**, which will generate relevant answers based on the retrieved document segments. \n",
    "\n",
    "For demonstration, we define a sample query: *\"What is the purpose of this Regulation?\"*\n",
    "\n",
    "Using this query, the following steps are performed:\n",
    "1. **Retrieve Relevant Documents**: We query the knowledge vector database to retrieve the most relevant chunks related to the question, aggregating them into a contextualized format.\n",
    "2. **Generate Answer**: The reader model processes the retrieved context to produce a concise, relevant answer to the query, which is then displayed.\n",
    "\n",
    "This setup creates a streamlined pipeline for asking questions and getting precise, document-based answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 09:59:54,906 - INFO - Knowledge vector database loaded from ../embeddings/knowledge_vector_database.faiss\n",
      "2024-11-03 09:59:56,738 - INFO - Reader LLM model 'Qwen/Qwen2.5-1.5B-Instruct' initialized successfully.\n",
      "2024-11-03 09:59:56,741 - INFO - Starting retrieval for query: How are general-purpose AI models defined in the AI Act?\n",
      "2024-11-03 09:59:56,742 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-11-03 09:59:56,745 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: General-purpose AI models are defined according to Article 51 of Chapter V's Section 1 within the AI Act. They must meet certain key functional characteristics, specifically being capable of performing widely diverse tasks across different domains. Additionally, there are evaluation standards outlined under Article 51 regarding impacts and capacities related to systematic risks, requiring assessment using appropriate technical tools and methodologies along with reference to specified guidelines detailed in Annex XIII.\n",
      "\n",
      "Pages retrieved from the document and included in the context for generating the answer: 90, 179, 285, 380\n"
     ]
    }
   ],
   "source": [
    "# Load the knowledge vector database\n",
    "knowledge_vector_database = load_knowledge_vector_database(FAISS_INDEX_PATH)\n",
    "if knowledge_vector_database is None:\n",
    "    logging.error(\"Failed to load the knowledge vector database.\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the reader model\n",
    "reader_llm, tokenizer = initialize_reader_model()\n",
    "\n",
    "# Define the query\n",
    "query = \"How are general-purpose AI models defined in the AI Act?\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs, context, retrieved_docs_metadata = retrieve_relevant_docs(query, knowledge_vector_database)\n",
    "\n",
    "# Generate the answer\n",
    "answer = generate_answer_from_docs(query, context, reader_llm, tokenizer, retrieved_docs_metadata)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
